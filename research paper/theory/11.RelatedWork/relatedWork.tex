In this section we share a rough overview of the principal topics that inspired this paper, and that are also on-going and related research subjects. The main concepts discussed are: \textit{active automata learning}, \textit{incremental minimal construction of deterministic finite automata} and \textit{graph matching}. \\ \\
%
Despite the fact that we do not interact directly with models in our incremental learning approach, we do share similar challenges that apply to active automata learning. There is no straightforward pattern that clearly indicates the termination and correctness of learning, as seen in the paper from Howar, Falk and Steffen, Bernhard \cite{activeLearning}. In active learning, it is possible to indicate partial correctness, because learning terminates after the equivalence is guaranteed by the execution of equivalence queries. As for termination, the amount of time to learn a system remains to be proved, because the learning process highly depends on queries to the system, which execution may vary in time depending on the received observations of a system. It is to be remarked that in our incremental learning approach we are also not able to determine full correctness and termination, due to the lack of interaction with the system and the types of observation that we evaluate. Nevertheless, there exists papers in which active learning is used with simple practical examples, as seen in paper written by Steffen, Bernhard and Howar, Falk and Merten, Maik \cite{activeAtuomata}, where models are learned not only with automata, but with \textit{Mealy machines}.\\ \\
%
Incremental learning and construction of automata has been also discussed and applied to improve the efficiency and optimization of automata.
%
Our approach consists on incremental learning and measurement of probabilistic timed automaton, which is strongly related to the approaches of constructing incremental minimal automata from the book of \textit{Optimization of automata } \cite{optimizationOfAutomata}. In this book, incremental construction of minimal finite-state automata is applied in the context of natural language processing by incrementally verifying that letters of a language are recognized by the minimal possible number of states. We do not ensure in our implementation that our \textit{learned models} are minimal, but we are capable of avoiding redundant nodes by verifying the similarity of the nodes with Euclidean distances and cost functions. Minimization of automata is also a very important and related topic, because minimal automata might be easier to learn as automata with redundant nodes. There exists a great variety of algorithms for automata minimization, which are very well described in the paper \textit{Minimization of automata} \cite{minimizationOfautomata}. \\ \\
%
Graph matching, as discussed in \Cref{chap:background}, is a powerful technique that involves matching nodes and edges of two graphs. We utilize similar techniques related to graph matching like \textit{semantic matching}, which can be seen in more detailed in the paper of \textit{S-Match: an algorithm and an implementation of semantic matching} \cite{graphSemanticMatching}. In graph semantic matching, the nodes of a graph are identified by calculating the semantic relations between concepts of nodes and not their labels. This is a very interesting concept that we applied by labeling nodes depending on their functionalities, with the use of Euclidean distances. 
%
Graph matching itself is a very wide and complicated topic that has been applied in many areas of the real world. One example can be web search engines, in which nodes and edges of enormous graphs are coupled by relating and categorizing sources or information of graphs by hubs and authorities, explained in paper the paper of \textit{Graph similarity scoring and matching} \cite{graphSimilarityScoringMatching}. Other interesting areas are: image analysis, biometric identification, video analysis, document processing, among others, very well explained in the paper of \textit{Thirty years of graph matching in pattern recognition} \cite{patternRecognition}. 
%
